Software defect prediction model helps in early detection of defects using classification technique. There is an ongoing discussion about when to use which metrics, algorithms, repository features and so on, for building classification models. We're not experts in the field but considering our experiment as well several articles and papers, we strongly feel there is no general answer on such questions. To properly document when to implement which approach, more experiments needs to be carried out, and possibly in a more organized manner, since many works don't demonstrate any clear pattern in their findings.\\
From our own results, we can say that decision trees perform slightly better for 2 out of the 3 metrics we used(correctness and AUC). We verified this with a paired t-test. \\ For our experiment with different number of metrics we, unfortunately cannot see any clear answer in the results. We cannot say that the number of metrics does not affect the performance of the prediction model at all, but we can also not say that it does. In order to confirm or deny this, a bigger experiment should be conducted.  On the other side, the pattern where Bayesian learners performed better using restricted set of repository metrics rather than the whole dataset has been replicated successfully. 
We also agree with Sathyaraj's and Prabu's claim\cite{sathyaraj2015approach} that Random Forest's perform better on larger datasets. Random Forest's performance rised with increasing number of repository metrics used. There was also almost 10\% and 5\% increase in F-measure and AUC respectively when applied on larger PC1 rather than small CM1 dataset. On the other side, similar difference between these 2 datasets was observed with other algorithms as well.