Software defect prediction model helps in early detection of defects using classification technique. There's an ongoing discussion about when to use which metrics, algorithms, repository features and so on for building classification models. We're not experts in the field but from our experiment as well as from read articles and papers, we strongly feel there is no general answer on such questions. To properly document when to implement which approach, more experiments need to be carried out (and possibly in a more organized manner), since many works don't demonstrate any clear pattern in their findings. Another argument we found reasonable and strong is the implementation of NOD(number of developers) metric as proposed in 

From our own results, we can say that decision trees perform slightly better for 2 out of 3 metrics we used(correctness and AUC).  We verified this with a paired t-test. We also agree with Sathyaraj's and Prabu's claim\cite{sathyaraj2015approach} that Random Forest's perform better on larger datasets. Random Forest's performance rised with increasing number of repository metrics used. There was also almost 10\% and 5\% increase in F-measure and AUC respectively when applied on larger PC1 rather than small CM1 dataset. On the other side, similar difference between these 2 datasets was observed with other algorithms as well.