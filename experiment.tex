Since we want to gain from our experiment as much knowledge from SFP field as possible, we've decided to carry out rather simple tests of several decision tree and bayesian algorithms to see how they perform on NASA MDP data sets. We carried tests with the whole datasets as well as with data sets restricted just on chosen code metrics. To deal with the lack of data but still avoid overfitting, we're mostly using K-fold cross validation approach. Here is needed to mention, that in WEKA, K-fold validation is executed internally to estimate the generalization error and the output model is the model trained on the whole dataset (partial K-fold models are not shown to the user). We utilize this to also perform our own tests with the datasets where this is possible (due to their sufficient size).

\subsection{Evaluation metrics}
To compare models(classifiers) and evaluate their performance, there are several metrics which can be used. Of course, each metric carries different information and should be used in different situations. In our case (SFP) it can't be generalised which metrics are important, because it depends on the particular project and policy utilized within the project (should we rather pay attention to the false positives or the false negatives - in other words, do we rather want our test team to concentrate of defect-less but falsely classified class or is it worse when testing team doesn't spend enough time with the defect-rich class because it has been classified as "safe"). Since we can't favor precision over recall or vice versa, F-measure offers itself as good way to consider both of them. We have also again taken look into Malhotra's article\cite{malhotra2015systematic} to find out that the most used ones are usually recall, precision, area under the curve(AUC - the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example\cite{japkowicz2011evaluating}) and F-measure. WEKA outputs the whole confusion matrix plus several more metrics so we utilized it to compare trained classifiers with several of these. 