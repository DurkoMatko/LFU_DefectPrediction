\section{Experiment description}
The field of Software Fault Prediction is big, and the number of possible and interesting experiments is very large. One could for example compare groups of classifiers (decision trees vs. Bayesian learners) or particular classifiers within one group. Comparison can also be to analyse performance on different datasets or compare particular software metrics. Another option could also be just changing the parameters of one classifier in the stable environment.\\\\
To build the models, we used all decision tree as well as Bayesian network algorithms available in WEKA\footnote{More about weka: \url{http://www.cs.waikato.ac.nz/ml/weka/}}. Reason behind our decision to use these two groups is that in most papers we worked with, decision trees and Bayesian networks have always been ranked among the top performing algorithms(e.g. \cite{malhotra2015systematic} and \cite{shivaji2009reducing}). Therefore the first question we addressed in our work was: \\
\textbf{Is there a significant difference in performance between decision trees and bayesian learners?}\\
We also tried to make our secondary experiment as similar as possible to real-world problems and experiments in the field. We found the topic of number of used metrics in a fault prediction discussed in Wang's paper\cite{wang2011many} interesting and therefore decided to also investigate this area. We decided to try and answer the following question:\\
\textbf{Does the number of metrics in a fault prediction model, change the performance of the predictor?}
\section{Experimental Setup}
We designed our experiment so that we compare two different groups of classifiers which we choose to be decision trees and bayesian algorithms and we decided to use NASA MDP PC1 dataset, which has been used in many cases. For each of these classifiers we are experimenting with 1,3,5 and 7 software repository metrics (numbers in \cite{wang2011many} where in the same range), which we will describe below. To deal with the lack of data but still avoid overfitting, we will mostly be using 10-fold cross validation approach. Here it is important to mention, that in WEKA, K-fold validation is executed internally to estimate the generalization error and the output model is the model trained on the whole dataset (partial K-fold models are not shown to the user). We utilize this to also perform our own tests with the datasets where this is possible (due to their sufficient size).
\section{Evaluation metrics}
To compare models and evaluate their performance, there are several metrics which can be used. Of course, each metric carries different information and should be used in different situations. In our case (SFP) it is important to notice which metrics are important and which are not, because it depends on the particular project and policy utilized within the project.
For example is it important to decide if a team should focus on false positives or false negatives. In other words, is it worse for the test team to concentrate on defect-less but falsely classified classes or is it worse when testing teams does not spend enough time with the defect-rich classes because it was classified as "safe". Because of this we can not favor \textit{precision} over \textit{recall} or vice versa. In such case, \textit{F-measure} offers itself as a good way to consider both of them. Additionaly, in Malhotra's \cite{malhotra2015systematic} as well as in Catal's\cite{catal2012performance} articles, one of the most used evaluation metrics was \textit{area under the curve} so we're using that one as well. At last, we've also decided to compare the correctness of the classifiers. Despite it's not a right metric to use\cite{sankar2014prediction,catal2012performance} with skewed datasets, it's intuitive and very easy to understand. Morover, despite not being ideal, it's still widely used in many works\cite{sahana2013software,prasad2015study,sharma2016software}. \\\\
\textit{F-measure} is the harmonic mean of precision and recall.\cite{malhotra2015systematic}\\\\
\textit{Area under the curve} or AUC is the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example.\cite{japkowicz2011evaluating}. It can be regarded as a measure of aggregated classification performance as it in some sense averages performance over all possible thresholds\cite{dejaeger2013toward}. \\\\
For each metric we have run a t-test to evaluate the significance of results. This process is described in more detail in section \ref{sec:testing}. 