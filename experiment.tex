\subsection{Experiment description}
The field of Software Fault Prediction is big, and the number of possible and interesting experiments is very large. One could for example compare groups of classifiers (decision trees vs. Bayesian learners) or particular classifiers within one group. Comparison can also be to analyse performance on different datasets or compare particular software metrics. Another option could also be just changing the parameters of one classifier in the stable environment.\\\\
We want to make an experiment that is relevant and useful and as mentioned in \ref{}\mytodo{find reference to article that mentiones that number of metrics could be interesting. was it malotra} the number of metrics in a fault prediction model might have an important influence on the results and is interesting to investigate. \\ We therefore decided to try and answer the following question:\\\\
\textbf{Does the number of metrics in a fault prediction model, change the precicion of the predictor?}\\
\subsection{Experimental Setup}
We designed our experiment so that we compare two different groups of classifiers which we choose to be decision trees and bayesian algorithms and we decided to use NASAs MDP PC1 dataset, which have been used in many cases. For each of these classifiers we are experimenting with 1,3,5 and 7 software repository metrics \mytodo{why was it we choose this number - because of an article I believe, so make reference}, which we will describe below. To deal with the lack of data but still avoid overfitting, we will mostly be using 10-fold cross validation approach. Here it is important to mention, that in WEKA, K-fold validation is executed internally to estimate the generalization error and the output model is the model trained on the whole dataset (partial K-fold models are not shown to the user). We utilize this to also perform our own tests with the datasets where this is possible (due to their sufficient size).
\subsection{Evaluation metrics}
To compare models and evaluate their performance, there are several metrics which can be used. Of course, each metric carries different information and should be used in different situations. In our case (SFP) it is important to notice which metrics are important and which are not, because it depends on the particular project and policy utilized within the project.
For example is it important to decide if a team should focus on false positives or false negatives. In other words, is it worse for the test team to concentrate on defect-less but falsely classified classes or is it worse when testing teams does not spend enough time with the defect-rich classes, because it was classified as "safe". \\
In Malhotra's article\cite{malhotra2015systematic} it is clear that the most used metrics are \textit{recall}, \textit{precision}, \textit{area under the curve} and \textit{F-measure}. \\\\
\textit{Recall} is the proportion of correctly predicted fault prone classes amongst all actual fault prone classes.\cite{malhotra2015systematic}\\\\
\textit{precision} is the proportion of correctly classified fault prone classes amongst total number of classified fault prone classes.\cite{malhotra2015systematic}\\\\
\textit{F-measure} is the harmonic mean of precision and sensitivity.\cite{malhotra2015systematic}\\\\
\textit{Area under the curve} or AUC is the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example.\cite{japkowicz2011evaluating}\\\\
Since we can not favor \textit{precision} over \textit{recall} or vice versa, \textit{F-measure} offers itself as a good way to consider both of them.\\
WEKA outputs the whole confusion matrix plus several more metrics so we utilize this to compare trained classifiers with several of these metrics. \\\\
We decided that it was interesting to look at the difference in the resulting metrics from WEKA and therefore we are comparing the mean results of the prediction models of \textit{Correctness}, \textit{F-meassure} and \textit{AUC}.\\\\
For each of these results we are creating a Hypothesis Test to evaluate the results. For the Hypothesis test we will use t-test, which also will be explained further, below. 