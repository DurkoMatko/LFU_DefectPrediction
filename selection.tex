In this section we describe the reasons behind choosing specific code metrics, classifiers as well as datasets to work with. The article \textit{Systematic Review of Machine Learning Techniques for Software Fault Prediction}\cite{malhotra2015systematic} from Ruchika Malhotra explains a lot about the metrics and classifiers and we have based most of our decisions on this article.
\section{Metrics}
A typical software defect prediction model is trained by using previously collected fault data. Metrics characteristics or count influences the performance of the model.\cite{wang2011many}. To decide the ideal number of metrics for the experiment, we looked at the results from Wang's paper on this topic\cite{wang2011many}. Here it can be seen that models perform very good with just 3 metrics. The paper also demonstrates that models generally perform better with restricted number of metrics then with many(all) of them. There are several metric groups used to describe software repositories and code itself. Besides traditional McCabe and Halstead static metrics, there are also Object-oriented metrics(cohesion, coupling and inheritance count), miscellaneous metrics(change and requirement metrics, code churn) or even hybrid metrics combining various metrics from previous groups. Malhotre did extrensive research in her paper and found that in more than 50\% of the 64 considered papers and articles on fault prediction, traditional procedural metrics had been used. Object-oriented metrics had also been used a lot\cite{radjenovic2013software}, particularly coupling between objects(CBO) and response for class(RFC) emerged as highly useful. On the other side, number of children(NOC) and depth of inheritance tree(DIT) didn't seem as good metrics for SFP purposes\cite[p.~15]{malhotra2015systematic}.\\\\
For our experiment, as mentioned above we decided to experiment with the number of metrics used in the fault prediction model. The metrics we decided to use are:\\
\begin{itemize}
\item Cyclomatic complexity: Is a count of linearly independent paths through a program's source code. 
\item Lines of code: Measure the size of a program by counting the number of lines in the text of the source code.
\item Branch count: Number of branches in project 
\item Number of unique operands: The number of unique operands such as numerical, text and boolean values.
\item Number of unique operators: The number of unique operators that evaluate the operands. Such could be plus, minus, multiply and divide. 
\item Halstead content: Rather complex metric representing algorithms complexity. It's affected by several other metrics\footnote{More on Halstead content: \url{https://maisqual.squoring.com/wiki/index.php/Halstead_Intelligent_Content}}
\item Maintenance severity: Describes difficulty of maintaining a module
\end{itemize}
It's been shown that models performing well seem to be using sets/combinations of various metrics (e.g. combinations of process, product and people-based metrics)\cite{hall2012systematic}. Based on this knowledge, we tried to use both McCabe and Halstead groups and generally tried to use more complex metrics so as many as possible repository features could make an impact. But despite this, metrics have been chosen randomly and there remains a research to be done in this field as well (to see which exact metrics lead to better performance).
\section{Classifier}
Choosing the right classifier is also not a straightforward process since there are many statistics and machine learning methods to be used and to combine. During the last ten years, many papers and articles have been published on the topic of classifier performance in the SFP field. According to Malhotra\cite{malhotra2015systematic},the most widely used classifiers are decision trees and Bayesian learners followed by neural networks and support vector machines\cite[p.~11]{malhotra2015systematic}.\\
It is because of this, that we decided to use decision trees and bayesian learners, since we wish to get the best possible results in order to properly evaluate the difference between the number of metrics.
\section{Datasets}
Several dataset options are available for SFP studies, but NASAs Metrics Data Program data sets are definitely the most widely used. In her research, Malhotra\cite{malhotra2015systematic} claims that they are used in more than 60\% of the 64 examined studies. The data set called KC1 is the most widely used (both module and class level) which is applied in 43\% of studies and
has 15.4\% faulty modules and 40.6\% faulty classes. Similarly, the PC1 data set is used in 40\% of studies and contains only 6.8\% of faulty modules.\cite[p.~17]{malhotra2015systematic}. For our experiment we decided to use the PC1 dataset due to its bigger size and therefore more training data. 